---
layout: new_post
title: Cat In The Box
subtitle: Reinforcement Learning with Stable Baselines
author: Ambarish Prakash
---

Outside of programming, one of my biggest passion's is playing Boardgames. What better way than to combine the two? In this two part series I want to digitalize of the fun boardgames that I enjoy, namely Cat In The Box. Now, over time I want to be able to create a website where you can play this game with friends online. However to add to that, I also wanted to play around with some RL and create an AI that could play this game as well. In this post, I talk about my experiments creating an AI that can play Cat In The Box well, by training it only using RL!


# Overview

Cat in the Box, designed by Muneyuki Yokouchi, is a fun trick taking game that emulates the concept of Schrodinger's cat - unless you actually look in the box you do not know whether it is alive or dead. This game twists the concept giving you only numbered cards, and then making you choose the suit(red, blue, yellow or green) only when it is played. If you want to follow the specific rules, you can find them over [here.](https://pegasus.de/media/pdf/39/18/8f/4250231736483_en.pdf)

![Game Components]({{'/assets/img/citb/Box-Contents-Cat-In-The-Box.jpg' | relative_url }}){: .mx-auto.d-block : width="550" height="400"}

In this project, I'll be focusing directly on the 4 player version as it fixes the setup and makes learning a bit easier. The 40 cards (5 copies of the numbers 1-8) are randomly distributed among the 4 players. Each player then chooses and discards one of their cards, as well as makes a bet as to the number of sets they think they might win. Finally they play up to 8 'sets' where they play cards.

In this project, I want to train an agent to be able to play the entire game, including discarding cards, setting their bet and playing the sets.


> **Note:** This game is not truly deterministic. Unlike Chess, Go, Pong, etc. at a given point playing a particular move will not always lead to the same next state and is dependent on card distribution as well as other player actions. This adds some challenges and affects the best possible model that can be achieved. 

<br>

# Reinforcement Learning Overview - A short dive

Reinforcement learning is different from normal Deep Learning due to the fact that there is no fixed data set. Instead there is a way for the training to actual go through the different actions and dynamically generate new data to train on. Each time an action is taken, a new state and reward is given. The model then 

![Rollouts]({{'/assets/img/citb/rl_idea.jpg' | relative_url }}){: .mx-auto.d-block : width="550" height="350"}

The agent model is an ML model that given an observation, estimates the expected reward when taking different actions. By going through multiple (thousands/millions) observations / actions / rewards, an agent (ML model) is taught to learn which actions are beneficial and which are not as much. The agent can thus decide what the best action to take by checking the predicted rewards and maximizing the expected reward.

![RL Train]({{'/assets/img/citb/rl_train.jpg' | relative_url }}){: .mx-auto.d-block : width="750" height="250"}

<br>

# Project Setup

## Packages

For the training I used **stable-baselines3** along with **gymnasium environments**. Stable-baselines along with gymnasium give a very good way to not only collect different training rollouts, but provide the implementation of a large number of RL models to use out of the box. Behind the scenes these models use pytorch to setup and train the model, and logs progress and statistics using tensorboard. Instead of Tensorboard, I used WandB, which has an inbuilt integration with stable baslines, to collect, visualize and analyse the results.


## Gymnasium

I built my own Custom Environment on top of Gymnasium's base environment. To define the environment we first need an action space and an observation space.

**Action Space:**
![Action Space]({{'/assets/img/citb/action_space.jpg' | relative_url }}){: .mx-auto.d-block : width="750" height="250"}

As far as I see there are three distinct actions that a player can make: 
<ul>
    <li> Discard a card: At the start of the game, players are required to discard one of their 10 starting cards. Given there are 8 numbers, a player can discard one of 8 different cards.</li>
    <li> Set a bet: The players also have to make a bet of how many sets they think they are going to win. Either 1,2 or 3 in a 4 player game.</li>
    <li> Play a card on the board: This is the meat of the game. Players play a card from their hand, stating a color and placing their tile on the corresponding space on the game board. Eg: Play a 8, say the color is Blue and cover the Blue 8 spot on the board.</li>
</ul>

Overall that gives 43 different actions that the model can make. 8 can only be done in the first turn of the game, 3 in the second turn and the remaining 32 actions after that.

**Observation Space:**

Since the game is made fixed actions and numbers, I could use a multi discrete action space. I thought about this from a player who is playing the game's perpective. What information a player would have is what I would pass in to the agent as well. 


This includes the cards in the hand, cards played the board state and visible information of other players. In addition I added a turn number, as well as a one hot encoded 'phase' variable (100 for discard, 010 for bets and 001 for play cards) which could help with choosing actions for different phases.

![Observation Space]({{'/assets/img/citb/observation_space.jpg' | relative_url }}){: .mx-auto.d-block : width="750" height="400"}

The total gives a list of size 81 for the observation space. Stablebaselines also performs an optimization to convert each discrete value into one hot encoding vectors. Therefore a variable i, whose value can range from 0-10 is converted to a one hot encoded vector of size 11 before being sent to the model. Hence the final input to the model is of size 194.

 
 **Custom Environment:**
 
 The [Stable Baselines Guide](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) for using a custom environment is what I used to build environment. In short, in order to create your own environment, you need to implement 3 main features:
 <ol>
    <li>
        Initialization: Here you have to set the action and observation space using gymnasium spaces. As mentioned above, since all my actions and observations were fixed values, I could use Discrete valued spaces.
    </li>
    <li>
        Reset: In this function, the environment needs to be reset back to it's original state / starting state along with the initial observation. In my case, it was resetting the game and redistributing a new set of cards.
    </li>
    <li>
        Step: This is what needs to needs to happend to the environment when the agent takes an action. Based on what action is taken, the board state should be updated, and the new observation space, along with the reward as well as if the end status should be returned. Initially I set the reward to 0 for all actions except when the game was over (where the actual score was resturned). If any invalid action was played I returned a high negative value to stop the action from being taken.
    </li>
 </ol>



